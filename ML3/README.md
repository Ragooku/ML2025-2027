# Лабораторная работа 3. Линейная регрессия
## Задание
Перед выполнением лабораторной работы необходимо загрузить набор данных в соответствии с вариантом на диск

1. Написать программу, которая разделяет исходную выборку на обучающую и тестовую (training set, test set). Использовать стандартные функции (train_test_split и др. нельзя).
2. С использованием библиотеки scikit-learn обучить модель линейной регрессии по обучающей выборке пример
3. Проверить точность модели по тестовой выборке
4. Построить модель с использованием полиномиальной функции. Построить графики зависимости точности на обучающей и тестовой выборке от степени полиномиальной функции.
5. Построить модель с использованием регуляризации. На основе экспериментов подобрать параметры для регуляризации. Построить графики зависимости точности модели на обучающей и тестовой выборках от коэффициента регуляризации.
## Вариант 6
Concrete Compressive Strength

## Загрузка dataset
```
concrete_compressive_strength = fetch_ucirepo(id=165)

# data (as pandas dataframes)
X = concrete_compressive_strength.data.features
y = concrete_compressive_strength.data.targets
```
x - Features, Признаки. y - Targets, Цели

Осуществляется подгрузка с сервера. В данном варианте я рассматривал 2 степени полиномов.
```
degrees = range(1, 3) # Это создает диапазон [1, 2]
```
Степень 1 - это обычная линейная регрессия (без полиномиальных признаков)
Степень 2 - полиномиальная регрессия с квадратичными членами и взаимодействиями

Степени 1-2 проще интерпретировать физически, а 3-4 способствует переобучению модели

## 1. Разделение исходной выборки на обучающую и тестовую в соотношении (20/80)
В примере из документации sklearn под тестовую выборку выделяется 20% данных. Поступим так же: с помощью стандартной функции sample() из библиотеки pandas, которая возвращает случайную подвыборку строк, отбираем 80% строк датафрейма. Тогда в тестовую выборку test_df с помощью функции drop() заносим оставшиеся 20% строк исходного датафрейма.

80% - достаточно данных для качественного обучения. 20% - достаточно данных для надежной оценки.
```
X, y = load_and_prepare_data()

# 1) Разделение на тестовую и обучающие выборки

# Общее колличество наблюдений
n_samples = X.shape[0]

# Массив индексов от 0 до n_samples-1
indices = np.arange(n_samples)

# Перемешиваем индексы случайным образом для того чтобы избежать упорядочности данных
np.random.shuffle(indices)

# Применяем перемешанные индексы к X и y (DataFrame/Series)
X_shuffled = X.iloc[indices]
y_shuffled = y.iloc[indices]

# Разделяем перемешанные данные (80 - обучение 20 - тестирование)
train_size = int(n_samples * 0.8)

X_train = X_shuffled[:train_size] - обучающая выборка
X_test = X_shuffled[train_size:] - тестовая выборка

y_train = y_shuffled[:train_size]
y_test = y_shuffled[train_size:]

```
## 2. Модель линейной регрессии
Линейная регрессия - статистический метод моделирования зависимости между независимыми переменными (признаками) и зависимой переменной (целевой величиной).

С использованием библиотеки scikit-learn обучить модель линейной регрессии по обучающей выборке

Создаём объект model класса LinearRegression и применяем метод fit() для обучения модели.

Затем тестируем нашу модель на обучающей и тестовой выборке.

h(x) = w_0 + w_1x

w_new = w_old - aj(w_old)

R² (коэффициент детерминации) – это нормированная метрика, показывающая, какую долю изменчивости целевой переменной модель смогла объяснить

R² показывает:

1.0 - идеальное предсказание 

0.0 - модель не лучше среднего значения

< 0 - модель хуже простого среднего

```
from sklearn.linear_model import LinearRegression

regressor = LinearRegression().fit(X_train, y_train)

y_train_pred = regressor.predict(X_train)
y_test_pred = regressor.predict(X_test)
```


![alt text](Screen_1.png)


## 3. Проверить точность модели по тестовой выборке
Проверка точности - это процесс оценки способности модели делать правильные предсказания на новых, ранее не виденных данных.

Для оценки качества модели используются метрики. Они помогают понять, насколько хорошо модель справляется с задачей предсказания. Мы можем использовать две основные метрики: MSE и R².

MSE (среднеквадратичная ошибка) измеряет абсолютное среднее отклонение предсказаний от реальных значений. Она удобна для сравнения моделей, работающих с одними единицами измерения, но ее значение сильно зависит от масштаба данных и чувствительно к выбросам.

R² (коэффициент детерминации) – это нормированная метрика, показывающая, какую долю изменчивости целевой переменной модель смогла объяснить. R² всегда находится в диапазоне от минус бесконечности до 1, где 1 означает идеальное предсказание. Благодаря своей нормированности, R² позволяет сравнивать модели независимо от масштаба данных, что делает его универсальным инструментом оценки.

Остановимся на использовании метрики R2. Для оценки модели по выбранной метрике будем использовать стандартную функцию r2_score. 

```
from sklearn.metrics import mean_squared_error, r2_score

print(f"Coefficient of determination TRAIN: {r2_score(y_train, y_train_pred):.2f}")
print(f"Coefficient of determination TEST: {r2_score(y_test, y_test_pred):.2f}")
```
Получаем удовлетворительные значения как для обучающей, так и для тестовой выборки. Модель обучена.

<p align="center">
  <img src="Screen_2.png" />
</p>

## 4.Построить модель с использованием полиномиальной функции
Создаём pipeline - объект, который автоматизирует процесс трансформации признаков в полиномиальные. Он будет применять шаги poly_features и linear_regression по порядку. Иначе нам пришлось бы вручную использовать методы

Полиномиальная регрессия — это метод машинного обучения, используемый для моделирования нелинейных зависимостей между переменными путем аппроксимации данных полиномом степени (k). В отличие от линейной регрессии, которая предполагает прямую связь, этот метод позволяет учитывать более сложные криволинейные тренды в данных, что делает его полезным в различных областях, таких как экономика и инженерия.
```
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures

degree = 2
alphas = np.logspace(-4, 3, 10)  # диапазон коэффициентов регуляризации

r2_train_list = []
r2_test_list = []

for alpha in alphas:
    pipeline = Pipeline([
        ("poly", PolynomialFeatures(degree=degree, include_bias=False)),
        ("scaler", StandardScaler()),
        ("ridge", Ridge(alpha=alpha, max_iter=10000))
    ])

    # Обучаем модель на train
    pipeline.fit(X_train, y_train)

    # Предсказания
    y_train_pred = pipeline.predict(X_train)
    y_test_pred = pipeline.predict(X_test)

    # R^2
    r2_train_list.append(r2_score(y_train, y_train_pred))
    r2_test_list.append(r2_score(y_test, y_test_pred))

plt.figure(figsize=(8, 5))
plt.semilogx(alphas, r2_train_list, marker='o', label="Train R^2")
plt.semilogx(alphas, r2_test_list, marker='o', label="Test R^2")
plt.xlabel("Alpha (коэффициент регуляризации)")
plt.ylabel("R^2")
plt.title(f"Ridge Regression (Polynomial degree={degree})")
plt.ylim(0, 1)
plt.grid(True)
plt.legend()
plt.show()
```
На графике мы наблюдаем переобучение (overfitting) модели.
![alt text](Screen_3.png)

## 5. Построить модель с использованием регуляризации
Ridge-регрессия (или гребневая регрессия) — это регуляризованная версия линейной регрессии, которая используется для борьбы с мультиколлинеарностью (линейной зависимостью между предикторами) и переобучением. Она добавляет к функции потерь штрафное слагаемое в виде квадрата коэффициентов, чтобы сделать веса модели меньше, но не обнулить их, в отличие от Lasso-регрессии.

Алгоритм обучения пытается найти баланс между подгонкой под данные и поддержанием коэффициентов на низком уровне.
```
degree = 2
alphas = np.logspace(-4, 3, 10)  # диапазон коэффициентов регуляризации

r2_train_list = []
r2_test_list = []

for alpha in alphas:
    pipeline = Pipeline([
        ("poly", PolynomialFeatures(degree=degree, include_bias=False)),
        ("scaler", StandardScaler()),
        ("ridge", Ridge(alpha=alpha, max_iter=10000))
    ])

    # Обучаем модель на train
    pipeline.fit(X_train, y_train)

    # Предсказания
    y_train_pred = pipeline.predict(X_train)
    y_test_pred = pipeline.predict(X_test)

    # R^2
    r2_train_list.append(r2_score(y_train, y_train_pred))
    r2_test_list.append(r2_score(y_test, y_test_pred))

plt.figure(figsize=(8, 5))
plt.semilogx(alphas, r2_train_list, marker='o', label="Train R^2")
plt.semilogx(alphas, r2_test_list, marker='o', label="Test R^2")
plt.xlabel("Alpha (коэффициент регуляризации)")
plt.ylabel("R^2")
plt.title(f"Ridge Regression (Polynomial degree={degree})")
plt.ylim(0, 1)
plt.grid(True)
plt.legend()
plt.show()

best_index = np.argmax(r2_test_list)
best_alpha = alphas[best_index]

print(f"Наилучший alpha: {best_alpha:.4f}")
```
Для борьбы с переобучением и коррелированными признаками применим регуляризацию Ridge (L2). Она добавляет штраф к сумме квадратов коэффициентов модели, что стабилизирует обучение и уменьшает влияние сильно коррелированных признаков.

Почему я использовал Ridge? 
1. Полиномиальные признаки сильно коррелированы между собой
2. Ridge лучше справляется с мультиколлинеарностью
3. Сохраняет все признаки

В качестве признаков используем полиномиальные признаки 3-й степени, как в предыдущем пункте (средняя точность и меньше затрат в вычислительной мощности). Обновим наш pipeline, который теперь выполняет следующие шаги:

PolynomialFeatures(degree=3, include_bias=False) – создаёт новые признаки, учитывающие комбинации и степени всех признаков (параметров смеси).

StandardScaler() – нормализует признаки. Это важно, так как L2-регуляризация чувствительна к масштабу признаков.

Ridge(alpha=α, max_iter=10000) – линейная регрессия с L2-регуляризацией. Параметр α контролирует силу штрафа: чем выше α, тем сильнее регуляризация и меньше риск переобучения.

Для подбора оптимального значения α выбираем диапазон от 10⁻⁴ до 10³ в логарифмической шкале.На графике можно увидеть, при каком αlpha модель лучше всего балансирует между переобучением и недообучением. Оптимальное значение α соответствует максимальному R² на тестовой выборке.


![alt text](Screen_4.png)

<p align="center">
  <img src="Screen_5.png" />
</p>
